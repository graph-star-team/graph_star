{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from node2vec import Node2Vec\n",
    "from torch_geometric.data import Data\n",
    "from utils.gsn_argparse import str2bool, str2actication\n",
    "import torch_geometric.utils as gutils\n",
    "from torch_geometric.nn import GAE\n",
    "import trainer\n",
    "import utils.gsn_argparse as gap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and make dataframes\n",
    "name = ['entity', 'id']\n",
    "entity_id = pd.read_csv('./data/FB15k/entities.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "all_entities = entity_id['entity'].values\n",
    "\n",
    "name = ['relation', 'id']\n",
    "relation_id = pd.read_csv('./data/FB15k/relations.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "all_relations = relation_id['relation'].values\n",
    "\n",
    "# Read RDF Triples\n",
    "name = ['subject', 'object', 'relation']\n",
    "data = pd.read_csv('./data/FB15k/valid.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "\n",
    "SUBSAMPLE = 100\n",
    "\n",
    "subjects = data['subject'].values\n",
    "objects = data['object'].values\n",
    "relations = data['relation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit entity encoder\n",
    "le_entity = LabelEncoder()\n",
    "le_entity.fit(all_entities)\n",
    "\n",
    "# fit relationship encoder\n",
    "le_relation = LabelEncoder()\n",
    "le_relation.fit(all_relations)\n",
    "\n",
    "# string list to int array using LabelEncoder on complete data set\n",
    "subjects = le_entity.transform(subjects)\n",
    "objects = le_entity.transform(objects)\n",
    "relations = le_relation.transform(relations)\n",
    "\n",
    "# encode subsample (change range to 0-N)\n",
    "le_entity2 = LabelEncoder().fit(np.append(subjects,objects))\n",
    "le_relation2 = LabelEncoder().fit(relations)\n",
    "\n",
    "\n",
    "subjects = le_entity2.transform(subjects)\n",
    "objects = le_entity2.transform(objects)\n",
    "relations = le_relation2.transform(relations)\n",
    "\n",
    "\n",
    "edge_attributes = torch.tensor(relations, dtype=torch.float)\n",
    "edge_index = torch.tensor([subjects, objects], dtype=torch.long)\n",
    "unique_entities = torch.tensor(np.unique(edge_index.reshape(edge_index.shape[-1]*2, 1)), dtype=torch.float)\n",
    "dataset = Data(x=unique_entities, edge_attr=edge_attributes, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[50000], edge_index=[2, 50000], x=[13292, 16])\n"
     ]
    }
   ],
   "source": [
    "dataset.x = torch.tensor(embedded_nodes.vectors, dtype=torch.float)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. unique relations: 916\n",
      "no. edge_type size: torch.Size([50000])\n",
      "no. relation size: (50000,)\n",
      "edge_index size: torch.Size([2, 50000])\n",
      "min: 0\n",
      "min: 915\n"
     ]
    }
   ],
   "source": [
    "data = dataset\n",
    "data.edge_type = torch.LongTensor(relations) #torch.zeros(((data.edge_index.size(-1)),)).long()\n",
    "data.batch = torch.zeros((1, data.num_nodes), dtype=torch.int64).view(-1)\n",
    "data.num_graphs = 1\n",
    "num_features = dataset.x.shape[-1] \n",
    "relation_dimension = len(np.unique(relations))\n",
    "print(f\"no. unique relations: {relation_dimension}\")\n",
    "print(f\"no. edge_type size: {data.edge_type.size()}\")\n",
    "print(f\"no. relation size: {relations.shape}\")\n",
    "print(f\"edge_index size: {data.edge_index.size()}\")\n",
    "print(f\"min: {np.min(relations)}\")\n",
    "print(f\"min: {np.max(relations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    # Read data and make dataframes\n",
    "    name = ['entity', 'id']\n",
    "    entity_id = pd.read_csv('./data/FB15k/entities.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "    all_entities = entity_id['entity'].values\n",
    "\n",
    "    name = ['relation', 'id']\n",
    "    relation_id = pd.read_csv('./data/FB15k/relations.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "    all_relations = relation_id['relation'].values\n",
    "\n",
    "    # Read RDF Triples\n",
    "    name = ['subject', 'object', 'relation']\n",
    "    data = pd.read_csv('./data/FB15k/valid.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "\n",
    "    SUBSAMPLE = 100\n",
    "\n",
    "    subjects = data['subject'].values\n",
    "    objects = data['object'].values\n",
    "    relations = data['relation'].values\n",
    "    # fit entity encoder\n",
    "    le_entity = LabelEncoder()\n",
    "    le_entity.fit(all_entities)\n",
    "\n",
    "    # fit relationship encoder\n",
    "    le_relation = LabelEncoder()\n",
    "    le_relation.fit(all_relations)\n",
    "\n",
    "    # string list to int array using LabelEncoder on complete data set\n",
    "    subjects = le_entity.transform(subjects)\n",
    "    objects = le_entity.transform(objects)\n",
    "    relations = le_relation.transform(relations)\n",
    "\n",
    "    # encode subsample (change range to 0-N)\n",
    "    le_entity2 = LabelEncoder().fit(np.append(subjects,objects))\n",
    "    le_relation2 = LabelEncoder().fit(relations)\n",
    "\n",
    "\n",
    "    subjects = le_entity2.transform(subjects)\n",
    "    objects = le_entity2.transform(objects)\n",
    "    relations = le_relation2.transform(relations)\n",
    "\n",
    "\n",
    "    edge_attributes = torch.tensor(relations, dtype=torch.float)\n",
    "    edge_index = torch.tensor([subjects, objects], dtype=torch.long)\n",
    "    unique_entities = torch.tensor(np.unique(edge_index.reshape(edge_index.shape[-1]*2, 1)), dtype=torch.float)\n",
    "    dataset = Data(x=unique_entities, edge_attr=edge_attributes, edge_index=edge_index)\n",
    "    \n",
    "    from gensim.models import KeyedVectors\n",
    "    embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')\n",
    "    dataset.x = torch.tensor(embedded_nodes.vectors, dtype=torch.float)\n",
    "    print(dataset)\n",
    "    \n",
    "    data = dataset\n",
    "    data.edge_type = torch.LongTensor(relations) #torch.zeros(((data.edge_index.size(-1)),)).long()\n",
    "    data.batch = torch.zeros((1, data.num_nodes), dtype=torch.int64).view(-1)\n",
    "    data.num_graphs = 1\n",
    "    num_features = dataset.x.shape[-1] \n",
    "    relation_dimension = len(np.unique(relations))\n",
    "    print(f\"no. unique relations: {relation_dimension}\")\n",
    "    print(f\"no. edge_type size: {data.edge_type.size()}\")\n",
    "    print(f\"no. relation size: {relations.shape}\")\n",
    "    print(f\"edge_index size: {data.edge_index.size()}\")\n",
    "    print(f\"min: {np.min(relations)}\")\n",
    "    print(f\"min: {np.max(relations)}\")\n",
    "    return dataset, le_entity, le_entity2, le_relation, le_relation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(sub, obj, rel):\n",
    "    sub = le_entity.inverse_transform(le_entity2.inverse_transform([sub]))\n",
    "    obj = le_entity.inverse_transform(le_entity2.inverse_transform([obj]))\n",
    "    rel = le_relation.inverse_transform(le_relation2.inverse_transform([rel]))\n",
    "    return sub[0], obj[0], rel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/m/07pd_j', '/m/02l7c8', '/film/film/genre')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_transform(data.edge_index[0][0], data.edge_index[1][0], data.edge_type[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict ={}\n",
    "for n1, n2, ys in zip(data.edge_index[0], data.edge_index[1], data.edge_type):\n",
    "    label_dict[int(n1), int(n2)] = int(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=True,\n",
    "                          cmap=\"Set2\")\n",
    "        nx.draw_networkx_edge_labels(h, pos=nx.spring_layout(h, seed=42), edge_labels=label_dict, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fafb7c00f946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\torch_geometric\\utils\\convert.py\u001b[0m in \u001b[0;36mto_networkx\u001b[1;34m(data, node_attrs, edge_attrs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\torch_geometric\\utils\\convert.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "G = to_networkx(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-72554d500022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(dataset.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you should probably not visualize huge node networks\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "if len(unique_entities) > 200:\n",
    "    print(\"you should probably not visualize huge node networks\")\n",
    "else:\n",
    "    visualize(G) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def freebase_parser(freebase_id):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    query = \\\n",
    "    '''SELECT ?sLabel WHERE { \n",
    "        ?s wdt:P646 \"''' + freebase_id + '''\".\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    LIMIT 1'''\n",
    "    res = get_results(endpoint_url, query)\n",
    "    if len(res['results']['bindings']) == 0:\n",
    "        return \"No result\"\n",
    "    else:\n",
    "        return get_results(endpoint_url, query)['results']['bindings'][0]['sLabel']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdf2txt(sub, obj, rel):\n",
    "    sub, obj, rel = inverse_transform(sub,obj,rel)\n",
    "    sub = freebase_parser(sub)\n",
    "    obj = freebase_parser(obj)\n",
    "    return sub, obj, str(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Racine',\n",
       " 'Lubbock',\n",
       " '/award/award_nominated_work/award_nominations./award/award_nomination/award')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf2txt(23, 2, 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c03de61a4e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnode2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use temp_folder for big graphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "# Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "node2vec = Node2Vec(G, dimensions=16, walk_length=15, num_walks=20, workers=1)  # Use temp_folder for big graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed nodes\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  \n",
    "# Any keywords acceptable by gensim.Word2Vec can be passed, \n",
    "# `dimensions` and `workers` are automatically passed\n",
    "# (from the Node2Vec constructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2Text(node_id):\n",
    "    freebase_id = le_entity.inverse_transform(le_entity2.inverse_transform([node_id]))\n",
    "    return freebase_parser(freebase_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for most similar nodes\n",
    "NODE_ID = '2'\n",
    "print(f\"Most similar Nodes to {node2Text(int(NODE_ID))}\")\n",
    "for node in model.wv.most_similar(NODE_ID):\n",
    "    sim_node_id, percentage = node\n",
    "    print(node2Text(int(sim_node_id)), percentage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for later use\n",
    "import os\n",
    "PATH = 'embeddings'\n",
    "NODE_EMBEDDING_NAME = \"node_embedding\"\n",
    "EMBEDDING_MODEL_NAME = \"node_embedding_model\"\n",
    "if not os.path.exists(PATH):\n",
    "    os.mkdir(PATH)\n",
    "model.wv.save_word2vec_format(os.path.join(PATH, NODE_EMBEDDING_NAME + \".kv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later use\n",
    "model.save(os.path.join(PATH, EMBEDDING_MODEL_NAME + \".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings \n",
    "embeddings = model.wv.load_word2vec_format(os.path.join(PATH, NODE_EMBEDDING_NAME + \".kv\"))\n",
    "embedded_model = model.wv.load(os.path.join(PATH, EMBEDDING_MODEL_NAME + \".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for most similar nodes\n",
    "NODE_ID = '2'\n",
    "print(f\"Most similar Nodes to {node2Text(int(NODE_ID))}\")\n",
    "for node in embedded_model.wv.most_similar(NODE_ID):\n",
    "    sim_node_id, percentage = node\n",
    "    print(node2Text(int(sim_node_id)), percentage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-c9768d42cce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedded_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embedded_x = torch.tensor(embeddings.vectors, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-f36b2849d581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.edge_index = torch.stack([row, col], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-3691167ed672>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGAE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\torch_geometric\\nn\\models\\autoencoder.py\u001b[0m in \u001b[0;36msplit_edges\u001b[1;34m(self, data, val_ratio, test_ratio)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;34m'batch'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m  \u001b[1;31m# No batch-mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = GAE.split_edges(GAE, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'test_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-a3d66273045f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_pos_edge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_pos_edge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Data' object has no attribute 'test_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "l1 = data.test_pos_edge_index[0][0]\n",
    "l2 = data.test_pos_edge_index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e53f0b27b37b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l1' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataset.edge_index[0].tolist().index(l1))\n",
    "print(dataset.edge_index[1].tolist().index(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(batch=[13292], edge_attr=[50000], edge_index=[2, 50000], edge_type=[50000], num_graphs=[1], x=[13292, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-1b862e394b4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5217\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10644\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y[np.where(dataset.edge_index.T == torch.tensor([5217, 10644]))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading FB15k training (valid file) data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_encode_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-42609ee015ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\tLoading FB15k training (valid file) data...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encode_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# create node embeddings if none exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_encode_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "\n",
    "entity_id = pd.read_csv('./data/FB15k/entities.txt', sep='\\t', header=None, names=['entity', 'id'], engine='python')\n",
    "entity = entity_id['entity'].values\n",
    "\n",
    "relation_id = pd.read_csv('./data/FB15k/relations.txt', sep='\\t', header=None, names=['relation', 'id'], engine='python')\n",
    "relation = relation_id['relation'].values\n",
    "\n",
    "data = pd.read_csv('./data/FB15k/valid.txt', sep='\\t', header=None, names=['subject', 'object', 'relation'], engine='python')\n",
    "print('\\tLoading FB15k training (valid file) data...')\n",
    "\n",
    "dataset = label_encode_dataset(entity, relation, data)\n",
    "\n",
    "# create node embeddings if none exists\n",
    "if not os.path.exists(\"embeddings\"):\n",
    "    create_node_embedding(dataset)\n",
    "embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')\n",
    "\n",
    "dataset.x = torch.tensor(embedded_nodes.vectors, dtype=torch.float)\n",
    "data = GAE.split_edges(GAE, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "full_length = dataset.edge_index.shape[-1]\n",
    "train_index = torch.tensor(dataset.edge_index[:, 0:m.floor(full_length*0.7)], dtype=torch.long)\n",
    "train_attr_index = torch.tensor(dataset.edge_attr[0:m.floor(full_length*0.7)], dtype=torch.long)\n",
    "\n",
    "val_index = torch.tensor(dataset.edge_index[:, m.floor(full_length*0.7):m.floor(full_length*0.9)], dtype=torch.long)\n",
    "val_attr_index = torch.tensor(dataset.edge_attr[m.floor(full_length*0.7):m.floor(full_length*0.9)], dtype=torch.long)\n",
    "\n",
    "test_index = torch.tensor(dataset.edge_index[:, m.floor(full_length*0.9):], dtype=torch.long)\n",
    "test_attr_index = torch.tensor(dataset.edge_attr[m.floor(full_length*0.9):], dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "dataset.edge_index = torch.cat([train_index, val_index, test_index], dim=1)\n",
    "dataset.edge_attr = torch.cat([train_attr_index, val_attr_index, test_attr_index])\n",
    "\n",
    "dataset.edge_train_mask = torch.cat([torch.ones((train_index.size(-1))),\n",
    "                                  torch.zeros((val_index.size(-1))),\n",
    "                                  torch.zeros((test_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_val_mask = torch.cat([torch.zeros((train_index.size(-1))),\n",
    "                                torch.ones((val_index.size(-1))),\n",
    "                                torch.zeros((test_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_test_mask = torch.cat([torch.zeros((train_index.size(-1))),\n",
    "                                 torch.zeros((val_index.size(-1))),\n",
    "                                 torch.ones((test_index.size(-1)))], dim=0).byte()\n",
    "\n",
    "dataset.edge_train_attr_mask = torch.cat([torch.ones((train_attr_index.size(-1))),\n",
    "                                  torch.zeros((val_attr_index.size(-1))),\n",
    "                                  torch.zeros((test_attr_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_val_attr_mask = torch.cat([torch.zeros((train_attr_index.size(-1))),\n",
    "                                torch.ones((val_attr_index.size(-1))),\n",
    "                                torch.zeros((test_attr_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_test_attr_mask = torch.cat([torch.zeros((train_attr_index.size(-1))),\n",
    "                                 torch.zeros((val_attr_index.size(-1))),\n",
    "                                 torch.ones((test_attr_index.size(-1)))], dim=0).byte()\n",
    "\n",
    "dataset.edge_type = torch.zeros(((dataset.edge_index.size(-1)),)).long()\n",
    "\n",
    "dataset.batch = torch.zeros((1, dataset.num_nodes), dtype=torch.int64).view(-1)\n",
    "dataset.num_graphs = 1\n",
    "num_features = dataset.x.shape[-1] \n",
    "num_relations = max(np.unique(dataset.edge_attr)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'val_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-acd23f9ef62f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_pos_edge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'val_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "np.where(dataset.edge_index.T == data.val_pos_edge_index.T[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'val_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-96523fa0b15f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_pos_edge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_pos_edge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Data' object has no attribute 'val_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "pair = dataset.val_pos_edge_index.T[0]\n",
    "obj = pair[1]\n",
    "sub = pair[0]\n",
    "rel = np.where(dataset.edge_index.T == data.val_pos_edge_index.T[0])[0]\n",
    "print(obj, sub, rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-5a208a4fb0ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdf2txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "rdf2txt(int(sub), int(obj), int(y[rel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(898.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_attr[4877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('American Pie', 'romance film', '/film/film/genre')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf2txt(8937, 4141, 341)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec\n",
    "import os\n",
    "\n",
    "def make_node_embeddings(dataset, path=\"embeddings\", node_embedding_name=\"node_embeddings\", embedding_model_name=\"node_embedding_model\", dimensions=16, walk_length=15, num_walks=20, workers=1, window=10, min_count=1, batch_words=4):\n",
    "    G = to_networkx(dataset)\n",
    "   \n",
    "    # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)  # Use temp_folder for big graphs\n",
    "    \n",
    "    # Embed nodes\n",
    "    model = node2vec.fit(window=window, min_count=min_count, batch_words=batch_words)  # Any keywords acceptable by gensim.Word2Vec can be passed, \n",
    "                                                                 # `dimensions` and `workers` are automatically passed\n",
    "                                                                 # (from the Node2Vec constructor)\n",
    "    \n",
    "    # Save embeddings for later use\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    model.wv.save_word2vec_format(os.path.join(path, node_embedding_name + \".kv\"))\n",
    "    model.save(os.path.join(path, embedding_model_name + \".pkl\"))\n",
    "    print(f\"Saved embedding and model in the {path} folder\")\n",
    "    return model.vw.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Graph Star Multi Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"output/FB15K_1024_Hid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphStar(\n",
       "  (fl): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (star_init): StarAttn(\n",
       "    (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv_list): ModuleList(\n",
       "    (0): GraphStarConv(256, 256, heads=4)\n",
       "    (1): GraphStarConv(256, 256, heads=4)\n",
       "    (2): GraphStarConv(256, 256, heads=4)\n",
       "  )\n",
       "  (star_attn_list): ModuleList(\n",
       "    (0): StarAttn(\n",
       "      (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): StarAttn(\n",
       "      (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): StarAttn(\n",
       "      (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (gcl1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (gcl2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (gcl3): Linear(in_features=128, out_features=0, bias=True)\n",
       "  (rl): Linear(in_features=918, out_features=256, bias=True)\n",
       "  (LP_loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[50000], edge_index=[2, 50000], x=[13292, 16])\n",
      "no. unique relations: 916\n",
      "no. edge_type size: torch.Size([50000])\n",
      "no. relation size: (50000,)\n",
      "edge_index size: torch.Size([2, 50000])\n",
      "min: 0\n",
      "min: 915\n"
     ]
    }
   ],
   "source": [
    "dataset, le_entity, le_entity2, le_relation, le_relation2 = load()\n",
    "\n",
    "def inverse_transform(sub, obj, rel):\n",
    "    sub = le_entity.inverse_transform(le_entity2.inverse_transform([sub]))\n",
    "    obj = le_entity.inverse_transform(le_entity2.inverse_transform([obj]))\n",
    "    rel = le_relation.inverse_transform(le_relation2.inverse_transform([rel]))\n",
    "    return sub[0], obj[0], rel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits embedding [13292, 256] -> [unique nodes x hidden layer]\n",
    "z = model.z\n",
    "# Edge index between 2 nodes\n",
    "edge_index = dataset.edge_index.T[5].T\n",
    "# relation type\n",
    "edge_type = dataset.edge_type[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=918, out_features=256, bias=True)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('comedian',\n",
       " 'Richard Dreyfuss',\n",
       " '/people/profession/people_with_this_profession')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf2txt(edge_index[0], edge_index[1], edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0461, -0.0483, -0.0591,  ..., -0.0494, -0.0815,  0.0566],\n",
       "        [-0.0573, -0.0042,  0.0070,  ..., -0.0586, -0.0540, -0.0835],\n",
       "        [-0.0400, -0.0135, -0.0529,  ..., -0.0017, -0.0051,  0.0231],\n",
       "        ...,\n",
       "        [ 0.0662,  0.0853,  0.0700,  ...,  0.0435, -0.0358, -0.0923],\n",
       "        [-0.0007, -0.0064,  0.0640,  ..., -0.0492, -0.0112,  0.0284],\n",
       "        [-0.0126,  0.0563, -0.0568,  ..., -0.0611, -0.0003, -0.0608]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updated in training (not after)\n",
    "model.RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = z[edge_index[0]]\n",
    "relation =  model.RW[edge_type]\n",
    "tail = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([918, 256])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.RW.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([918, 256])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING FUNCTION\n",
    "p = head*relation*tail\n",
    "score = torch.sigmoid(p.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([13292, 256])\n"
     ]
    }
   ],
   "source": [
    "print(head.size())\n",
    "print(relation.size())\n",
    "print(tail.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = score.detach().numpy().argsort()[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 285, 1352,  116, 6068, 9022, 6892,  664, 5441, 9574, 2999],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('comedian', 'Phil Collins', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Billy Bob Thornton', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Marlene Dietrich', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'January Jones', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Trinity College', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Alison Brie', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Hugo Weaving', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Primetime Emmy Award for Outstanding Writing for a Comedy Series', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'University of California, Los Angeles', '/people/profession/people_with_this_profession')\n",
      "('comedian', 'Jimmy Smits', '/people/profession/people_with_this_profession')\n"
     ]
    }
   ],
   "source": [
    "for l in pred:\n",
    "    print(rdf2txt(edge_index[0], l, edge_type))\n",
    "# Recall precision on top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits embedding [13292, 256] -> [unique nodes x hidden layer]\n",
    "z = model.z\n",
    "\n",
    "def experiment(index=0):\n",
    "    \n",
    "    # Edge index between 2 nodes\n",
    "    edge_index = dataset.edge_index.T[index].T\n",
    "    # relation type\n",
    "    edge_type = dataset.edge_type[index]\n",
    "    \n",
    "    h, t, r = rdf2txt(edge_index[0], edge_index[1], edge_type)\n",
    "    print(f\" \\\n",
    "        Original data: \\n \\\n",
    "        Head: {h} \\n \\\n",
    "        Relation: {r} \\n \\\n",
    "        Tail: {t} \\n\")\n",
    "    \n",
    "    head = z[edge_index[0]]\n",
    "    relation =  model.RW\n",
    "    tail = z[edge_index[1]]\n",
    "    p = head * relation * tail\n",
    "    pred = int(round(np.argmax(p.detach().numpy())/256))\n",
    "    \n",
    "    h, t, r = rdf2txt(edge_index[0], edge_index[1], pred)\n",
    "    print(f\" \\\n",
    "        Predicted data: \\n \\\n",
    "        Head: {h} \\n \\\n",
    "        Relation: {r} \\n \\\n",
    "        Tail: {t} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Original data: \n",
      "         Head: As Good as It Gets \n",
      "         Relation: /film/film/other_crew./film/film_crew_gig/film_crew_role \n",
      "         Tail: make-up artist \n",
      "\n",
      "         Predicted data: \n",
      "         Head: As Good as It Gets \n",
      "         Relation: /cvg/cvg_genre/games \n",
      "         Tail: make-up artist \n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment(1690)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = z[edge_index[0]]\n",
    "relation =  model.RW[edge_type]\n",
    "tail = z[edge_index[1]]\n",
    "p = head * relation * tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5769, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate\n",
    "All should use top 10 / 5 / 1\n",
    "*  Zero - One f0-score (does it predict correctly or not)\n",
    "*  Hierachical correctness (How far up the relationtree ? Does it get film/film or film/film/other_crew)\n",
    "*  Top-K hits (Is the correct in label in top-K in predictions) (precision, recall, f0)\n",
    "*  Split relationship and regex in pred relations (finding related relations)\n",
    "\n",
    "Need to explain hierachical layout of freebase!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 THE LINK PREDICTION PROBLEM\n",
    "This section provides a detailed outline for the LP task in the context of KGs, introducing key concepts that we are\n",
    "going to refer to in our work.\n",
    "\n",
    "\n",
    "  We define a KG as a labeled, directed multi-graph KG = (E, R, G):\n",
    "* E: a set of nodes representing entities;\n",
    "* R: a set of labels representing relations;\n",
    "* G  E  R  E: a set of edges representing _facts_ connecting pairs of entities. Each fact is a triple _h, r, t_, where\n",
    "_h_ is the head, _r_ is the relation, and _t_ is the tail of the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('comedian',\n",
       " 'Richard Dreyfuss',\n",
       " '/people/profession/people_with_this_profession')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logits embedding [13292, 256] -> [unique nodes x hidden layer]\n",
    "z = model.z\n",
    "# Edge index between 2 nodes\n",
    "edge_index = dataset.edge_index.T[5].T\n",
    "# relation type\n",
    "edge_type = dataset.edge_type[5]\n",
    "rdf2txt(edge_index[0], edge_index[1], edge_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link Prediction** (LP) is the task of exploiting the existing facts in a KG to infer missing ones. This amounts to\n",
    "guessing the correct entity that completes _h, r, ?_ (tail prediction) or _?, r, t_ (head prediction). For the sake of simplicity, when talking about head and tail prediction globally, we call _source_ entity the known entity in the prediction, and _target_ entity the one to predict.\n",
    "\n",
    "\n",
    "In the following we use _italic_ letters to identify KG elements (entities or relations), and **bold** letters to identify the\n",
    "corresponding embeddings. Given for instance a generic entity, we may use _e_ when referring to its element in the\n",
    "graph, and **e** when referring to its embedding.\n",
    "\n",
    "\n",
    "Datasets employed in LP research are typically obtained subsampling real-world KGs; each dataset can therefore be\n",
    "seen as a small KG with its own sets of entities E, relations R and facts G. In order to facilitate research, G is further\n",
    "split into three disjoint subsets: a training set G_train, a validation set G_valid and a test set G_test.\n",
    "\n",
    "\n",
    "Most of LP models based on embeddings define a scoring function  to estimate the plausibility of any fact _h, r, t_\n",
    "using their embeddings:\n",
    "\n",
    "\n",
    "> (**h** ,**r**, **t**)\n",
    "\n",
    "\n",
    "In this paper, unless differently specified, we are going to assume that the higher the score of , the more plausible the\n",
    "fact.\n",
    "\n",
    "\n",
    "During training, embeddings are usually initialized randomly and subsequently improved with optimization algorithms such as back-propagation with gradient descent. The positive samples in G_train are often randomly corrupted\n",
    "in order to generate negative samples. The optimization process aims at maximizing the plausibility of positive facts as\n",
    "well as minimizing the plausibility of negative facts; this often amounts to employing a **triplet loss** function. Over time,\n",
    "more effective ways to generate negative triples have been proposed, such as sampling from a Bernouilli distribution [66] or generating them with adversarial algorithms [55]. In addition to the embeddings of KG elements, models may also\n",
    "use the same optimization algorithms to learn additional parameters (e.g. the weights of neural layers). Such parameters,\n",
    "if present, are employed in the scoring function  to process the actual embeddings of entities and relations. Since they\n",
    "are not specific to any KG element, they are often dubbed _shared parameters_.\n",
    "\n",
    "\n",
    "In prediction phase, given an incomplete triple _h, r, ?_, the missing tail is inferred as the entity that, completing the\n",
    "triple, results in the highest score:\n",
    "> t = argmax_(eE) (**h**, **r**, **e**)\n",
    "\n",
    "Head prediction is performed analogously.\n",
    "\n",
    "Evaluation is carried out by performing both head and tail prediction on all test triples in G_test , and computing for\n",
    "each prediction how the target entity ranks against all the other ones. Ideally, the target entity should yield the highest\n",
    "plausibility.\n",
    "\n",
    "Ranks can be computed in two largely different settings, called raw and filtered scenarios. As a matter of fact, a prediction may have multiple valid answers: for instance, when predicting the tail for _ Barack Obama, parent, Natasha Obama _,\n",
    "a model may associate a higher score to _Malia Obama_ than to _Natasha Obama_. More generally, if the predicted fact is\n",
    "contained in G (that is, either in G_train, or in G_valid or in G_test ), the answer is valid. Depending on whether valid\n",
    "answers should be considered acceptable or not, two separate settings have been devised:\n",
    "\n",
    "*  _Raw Scenario_: in this scenario, valid entities outscoring the target one are considered as mistakes. Therefore\n",
    "they do contribute to the rank computation. Given a test fact h, r, t  Gtest , the raw rank r_t of the target\n",
    "tail _t_ is computed as:\n",
    "\n",
    "> _r_t = |{e  E \\ {t } : (h, r, e) > (h, r,t)}|_ + 1\n",
    "\n",
    "The raw rank in head prediction can be computed analogously.\n",
    "\n",
    "*  _Filtered Scenario_: in this scenario, valid entities outscoring the target one are not considered mistakes. Therefore\n",
    "they are skipped when computing the rank. Given a test fact _h, r, t  G_test_ , the filtered rank r_t of the target\n",
    "tail t is computed as:\n",
    "\n",
    "> _r_t = |{e  E \\ {t } : (h, r, e) > (h, r,t)  h h, r, e i < G}|_ + 1\n",
    "\n",
    "The filtered rank in head prediction can be computed analogously.\n",
    "\n",
    "\n",
    "In order to compute the rank it is also necessary to define the policy to apply when the target entity obtains the\n",
    "same score as other ones. This event is called a _tie_ and it can be handled with different policies:\n",
    "\n",
    "* _min_: the target is given the lowest rank among the entities in tie. This is the most permissive policy, and it may\n",
    "result in artificially boosting performances: as an extreme example, a model systematically setting the same\n",
    "score to all entities would obtain perfect results under this policy.\n",
    "*  _average_: the target is given the average rank among the entities in tie.\n",
    "*  _random_: the target is given a random rank among the entities in tie. On large test sets, this policy should\n",
    "globally amount to the average policy.\n",
    "*  _ordinal_: the entities in tie are given ranks based on the order in which they have been passed to the model.\n",
    "This usually depends on the internal identifiers of entities, which are independent from their scores: therefore\n",
    "this policy should globally correspond to the random policy.\n",
    "*  _max_: the target is given the highest (worst) rank among the entities in tie. This is the most strict policy.\n",
    "\n",
    "The ranks Q obtained from test predictions are usually employed to compute standard global metrics. The most\n",
    "commonly employed metrics in LP are:\n",
    "> _Mean Rank_ (MR). It is the average of the obtained ranks:\n",
    "\n",
    "\\begin{equation*}\n",
    "MR = \\frac{1}{|Q|} \\sum_{q\\in Q} q\n",
    "\\end{equation*}\n",
    "\n",
    "It is always between 1 and |E|, and the lower it is, the better the model results. It is very sensitive to outliers, therefore\n",
    "researchers lately have started avoiding it, resorting to Mean Reciprocal Rank instead.\n",
    "\n",
    "_Mean Reciprocal Rank_ (MRR). It is the average of the inverse of the obtained ranks:\n",
    "\n",
    "\\begin{equation*}\n",
    "MRR = \\frac{1}{|Q|} \\sum_{q\\in Q} \\frac{1}{q}\n",
    "\\end{equation*}\n",
    "\n",
    "It is always between 0 and 1, and the higher it is, the beer the model results.\n",
    "\n",
    "\n",
    "> _Hits@K_ (H@K). It is the ratio of predictions for which the rank is equal or lesser than a threshold _K_:\n",
    "\n",
    "\\begin{equation*}\n",
    "H@K = \\frac{|\\{q  Q : q  K\\}|}{|Q|}\n",
    "\\end{equation*}\n",
    "\n",
    "Common values for K are 1, 3, 5, 10. The higher the H@K, the better the model results. In particular, when K = 1, it\n",
    "measures the ratio of the test facts in which the target was predicted correctly on the first try. H@1 and MRR are often\n",
    "closely related, because these predictions also correspond to the most relevant addends to the MRR formula.\n",
    "\n",
    "\n",
    "These metrics can be computed either separately for subsets of predictions (e.g. considering separately head and tail\n",
    "predictions) or considering all test predictions altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distMult(h,r,t):\n",
    "    return h*r*t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lp_log(self, z, pos_edge_index, pos_edge_type, known_edge_index, known_edge_type):\n",
    "        dt, dev = pos_edge_index.dtype, pos_edge_index.device\n",
    "        ranks = []\n",
    "\n",
    "        # head batch\n",
    "        for i in range(len(pos_edge_type)):\n",
    "            pei = torch.stack([torch.arange(0, z.size(0), dtype=dt, device=dev),\n",
    "                               torch.full((z.size(0),), pos_edge_index[1][i], dtype=dt, device=dev)], dim=0)\n",
    "            pet = torch.full((z.size(0),), pos_edge_type[i], dtype=dt, device=dev)\n",
    "            pred = distMult(z, pei, pet)\n",
    "            rank = (pred >= pred[pos_edge_index[0][i]]).sum().item()\n",
    "\n",
    "            # filter\n",
    "            filter_idx = (\\\n",
    "                (known_edge_index[1] == pos_edge_index[1][i]) * (known_edge_type == pos_edge_type[i])\\\n",
    "            ).nonzero().view(-1)\n",
    "            \n",
    "            filter_idx = known_edge_index[0][filter_idx]\n",
    "            rank -= (pred[filter_idx] >= pred[pos_edge_index[0][i]]).sum().item()\n",
    "            rank += 1\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # tail batch\n",
    "        for i in range(len(pos_edge_type)):\n",
    "            pei = torch.stack([torch.full((z.size(0),), pos_edge_index[0][i], dtype=dt, device=dev),\n",
    "                               torch.arange(0, z.size(0), dtype=dt, device=dev)], dim=0)\n",
    "            pet = torch.full((z.size(0),), pos_edge_type[i], dtype=dt, device=dev)\n",
    "            pred = self.lp_score(z, pei, pet)\n",
    "\n",
    "            rank = (pred >= pred[pos_edge_index[1][i]]).sum().item()\n",
    "\n",
    "            # filter\n",
    "            filter_idx = ((known_edge_index[0] == pos_edge_index[0][i]) * (\n",
    "                    known_edge_type == pos_edge_type[i])).nonzero().view(-1)\n",
    "            filter_idx = known_edge_index[1][filter_idx]\n",
    "            rank -= (pred[filter_idx] >= pred[pos_edge_index[1][i]]).sum().item()\n",
    "            rank += 1\n",
    "            ranks.append(rank)\n",
    "        ranks = np.array(ranks)\n",
    "\n",
    "        print(\"MRR: %f, MR: %f, HIT@1: %f, HIT@3: %f, HIT@10: %f\" % (\n",
    "            (1 / ranks).sum() / len(ranks),\n",
    "            (ranks).sum() / len(ranks),\n",
    "            (ranks <= 1).sum() / len(ranks),\n",
    "            (ranks <= 3).sum() / len(ranks),\n",
    "            (ranks <= 10).sum() / len(ranks)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(358)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_type[12566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset):\n",
    "    df = pd.DataFrame([dataset.edge_index[0], dataset.edge_index[1], dataset.edge_type]).T\n",
    "    df = df.sample(frac=1)\n",
    "    edge_attributes = torch.tensor(list(df[2].values), dtype=torch.float)\n",
    "    edge_index = torch.tensor([list(df[0].values), list(df[1].values)], dtype=torch.long)\n",
    "    dataset.edge_type = edge_attributes\n",
    "    dataset.edge_index = edge_index \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame([dataset.edge_index[0], dataset.edge_index[1], dataset.edge_type]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8937.,  8520., 10625.,  ..., 11546.,  4182.,  5191.])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(list(k[0].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attributes = torch.tensor(list(k[2].values), dtype=torch.float)\n",
    "edge_index = torch.tensor([list(k[0].values), list(k[1].values)], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(batch=[13292], edge_attr=[50000], edge_index=[2, 50000], edge_type=[50000], num_graphs=[1], x=[13292, 16])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(804.)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_type[12566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4725,  1421,   221,  ..., 12100,  7107,  7038],\n",
       "        [ 5435,  6329,  3152,  ..., 10204,  4172,  6268]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset):\n",
    "    train_size = int(np.floor(dataset.num_nodes*0.7))\n",
    "    valid_size = int(np.floor(dataset.num_nodes*0.9))\n",
    "    # edge_indexes\n",
    "    dataset.train_pos_edge_index = dataset.edge_index.T[0:train_size].T\n",
    "    dataset.val_pos_edge_index = dataset.edge_index.T[train_size : valid_size].T\n",
    "    dataset.test_pos_edge_index = dataset.edge_index.T[valid_size:].T\n",
    "    \n",
    "   # relations\n",
    "    dataset.train_edge_type = dataset.edge_type[0:train_size]\n",
    "    dataset.val_edge_type = dataset.edge_type[train_size : valid_size]\n",
    "    dataset.test_edge_type = dataset.edge_type[valid_size:]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_val_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(batch=[13292], edge_attr=[50000], edge_index=[2, 50000], edge_type=[50000], num_graphs=[1], test_edge_type=[38038], test_pos_edge_index=[2, 38038], test_pos_edge_type=[38038], train_edge_type=[9304], train_pos_edge_index=[2, 9304], train_pos_edge_type=[9304], val_edge_type=[2658], val_pos_edge_index=[2, 2658], val_pos_edge_type=[2658], x=[13292, 16])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([324., 358., 597.,  ..., 270., 695.,  36.])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dataset, 'val_edge_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(edge_index, num_nodes=None, num_neg_samples=None):\n",
    "    r\"\"\"Samples random negative edges of a graph given by :attr:`edge_index`.\n",
    "\n",
    "    Args:\n",
    "        edge_index (LongTensor): The edge indices.\n",
    "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
    "        num_neg_samples (int, optional): The number of negative samples to\n",
    "            return. If set to :obj:`None`, will try to return a negative edge\n",
    "            for every positive edge. (default: :obj:`None`)\n",
    "\n",
    "    :rtype: LongTensor\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "    num_neg_samples = num_neg_samples or edge_index.size(1)\n",
    "\n",
    "    # Handle '2*|edges| > num_nodes^2' case.\n",
    "    num_neg_samples = min(num_neg_samples,\n",
    "                          num_nodes * num_nodes - edge_index.size(1))\n",
    "\n",
    "    idx = (edge_index[0] * num_nodes + edge_index[1]).to('cpu')\n",
    "\n",
    "    rng = range(num_nodes**2)\n",
    "    perm = torch.tensor(random.sample(rng, num_neg_samples))\n",
    "    mask = torch.from_numpy(np.isin(perm, idx)).to(torch.bool)\n",
    "    rest = mask.nonzero().view(-1)\n",
    "    while rest.numel() > 0:  # pragma: no cover\n",
    "        tmp = torch.tensor(random.sample(rng, rest.size(0)))\n",
    "        mask = torch.from_numpy(np.isin(tmp, idx)).to(torch.bool)\n",
    "        perm[rest] = tmp\n",
    "        rest = rest[mask.nonzero().view(-1)]\n",
    "\n",
    "    row, col = perm / num_nodes, perm % num_nodes\n",
    "    return torch.stack([row, col], dim=0).long().to(edge_index.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import structured_negative_sampling\n",
    "\n",
    "out = structured_negative_sampling(dataset.val_pos_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3885,  9831,  6594,  ...,   653, 11341,  7571],\n",
       "        [ 1439, 12421, 10456,  ...,  2402,  5407,  9019]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([list(out[0]), list(out[2])], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "cal_mrr_score = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode != \"train\") and cal_mrr_score:\n",
    "    print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
