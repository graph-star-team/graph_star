{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from node2vec import Node2Vec\n",
    "from torch_geometric.data import Data\n",
    "from utils.gsn_argparse import str2bool, str2actication\n",
    "import torch_geometric.utils as gutils\n",
    "from torch_geometric.nn import GAE\n",
    "import trainer\n",
    "import utils.gsn_argparse as gap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and make dataframes\n",
    "name = ['entity', 'id']\n",
    "entity_id = pd.read_csv('./data/FB15k/entities.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "all_entities = entity_id['entity'].values\n",
    "\n",
    "name = ['relation', 'id']\n",
    "relation_id = pd.read_csv('./data/FB15k/relations.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "all_relations = relation_id['relation'].values\n",
    "\n",
    "# Read RDF Triples\n",
    "name = ['subject', 'object', 'relation']\n",
    "data = pd.read_csv('./data/FB15k/valid.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "\n",
    "SUBSAMPLE = 100\n",
    "\n",
    "subjects = data['subject'].values\n",
    "objects = data['object'].values\n",
    "relations = data['relation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit entity encoder\n",
    "le_entity = LabelEncoder()\n",
    "le_entity.fit(all_entities)\n",
    "\n",
    "# fit relationship encoder\n",
    "le_relation = LabelEncoder()\n",
    "le_relation.fit(all_relations)\n",
    "\n",
    "# string list to int array using LabelEncoder on complete data set\n",
    "subjects = le_entity.transform(subjects)\n",
    "objects = le_entity.transform(objects)\n",
    "relations = le_relation.transform(relations)\n",
    "\n",
    "# encode subsample (change range to 0-N)\n",
    "le_entity2 = LabelEncoder().fit(np.append(subjects,objects))\n",
    "le_relation2 = LabelEncoder().fit(relations)\n",
    "\n",
    "\n",
    "subjects = le_entity2.transform(subjects)\n",
    "objects = le_entity2.transform(objects)\n",
    "relations = le_relation2.transform(relations)\n",
    "\n",
    "\n",
    "edge_attributes = torch.tensor(relations, dtype=torch.float)\n",
    "edge_index = torch.tensor([subjects, objects], dtype=torch.long)\n",
    "unique_entities = torch.tensor(np.unique(edge_index.reshape(edge_index.shape[-1]*2, 1)), dtype=torch.float)\n",
    "dataset = Data(x=unique_entities, edge_attr=edge_attributes, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[50000], edge_index=[2, 50000], x=[13292, 16])\n"
     ]
    }
   ],
   "source": [
    "dataset.x = torch.tensor(embedded_nodes.vectors, dtype=torch.float)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. unique relations: 916\n",
      "no. edge_type size: torch.Size([50000])\n",
      "no. relation size: (50000,)\n",
      "edge_index size: torch.Size([2, 50000])\n",
      "min: 0\n",
      "min: 915\n"
     ]
    }
   ],
   "source": [
    "data = dataset\n",
    "data.edge_type = torch.LongTensor(relations) #torch.zeros(((data.edge_index.size(-1)),)).long()\n",
    "data.batch = torch.zeros((1, data.num_nodes), dtype=torch.int64).view(-1)\n",
    "data.num_graphs = 1\n",
    "num_features = dataset.x.shape[-1] \n",
    "relation_dimension = len(np.unique(relations))\n",
    "print(f\"no. unique relations: {relation_dimension}\")\n",
    "print(f\"no. edge_type size: {data.edge_type.size()}\")\n",
    "print(f\"no. relation size: {relations.shape}\")\n",
    "print(f\"edge_index size: {data.edge_index.size()}\")\n",
    "print(f\"min: {np.min(relations)}\")\n",
    "print(f\"min: {np.max(relations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    # Read data and make dataframes\n",
    "    name = ['entity', 'id']\n",
    "    entity_id = pd.read_csv('./data/FB15k/entities.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "    all_entities = entity_id['entity'].values\n",
    "\n",
    "    name = ['relation', 'id']\n",
    "    relation_id = pd.read_csv('./data/FB15k/relations.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "    all_relations = relation_id['relation'].values\n",
    "\n",
    "    # Read RDF Triples\n",
    "    name = ['subject', 'object', 'relation']\n",
    "    data = pd.read_csv('./data/FB15k/valid.txt', sep='\\t', header=None, names=name, engine='python')\n",
    "\n",
    "    SUBSAMPLE = 100\n",
    "\n",
    "    subjects = data['subject'].values\n",
    "    objects = data['object'].values\n",
    "    relations = data['relation'].values\n",
    "    # fit entity encoder\n",
    "    le_entity = LabelEncoder()\n",
    "    le_entity.fit(all_entities)\n",
    "\n",
    "    # fit relationship encoder\n",
    "    le_relation = LabelEncoder()\n",
    "    le_relation.fit(all_relations)\n",
    "\n",
    "    # string list to int array using LabelEncoder on complete data set\n",
    "    subjects = le_entity.transform(subjects)\n",
    "    objects = le_entity.transform(objects)\n",
    "    relations = le_relation.transform(relations)\n",
    "\n",
    "    # encode subsample (change range to 0-N)\n",
    "    le_entity2 = LabelEncoder().fit(np.append(subjects,objects))\n",
    "    le_relation2 = LabelEncoder().fit(relations)\n",
    "\n",
    "\n",
    "    subjects = le_entity2.transform(subjects)\n",
    "    objects = le_entity2.transform(objects)\n",
    "    relations = le_relation2.transform(relations)\n",
    "\n",
    "\n",
    "    edge_attributes = torch.tensor(relations, dtype=torch.float)\n",
    "    edge_index = torch.tensor([subjects, objects], dtype=torch.long)\n",
    "    unique_entities = torch.tensor(np.unique(edge_index.reshape(edge_index.shape[-1]*2, 1)), dtype=torch.float)\n",
    "    dataset = Data(x=unique_entities, edge_attr=edge_attributes, edge_index=edge_index)\n",
    "    \n",
    "    from gensim.models import KeyedVectors\n",
    "    embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')\n",
    "    dataset.x = torch.tensor(embedded_nodes.vectors, dtype=torch.float)\n",
    "    print(dataset)\n",
    "    \n",
    "    data = dataset\n",
    "    data.edge_type = torch.LongTensor(relations) #torch.zeros(((data.edge_index.size(-1)),)).long()\n",
    "    data.batch = torch.zeros((1, data.num_nodes), dtype=torch.int64).view(-1)\n",
    "    data.num_graphs = 1\n",
    "    num_features = dataset.x.shape[-1] \n",
    "    relation_dimension = len(np.unique(relations))\n",
    "    print(f\"no. unique relations: {relation_dimension}\")\n",
    "    print(f\"no. edge_type size: {data.edge_type.size()}\")\n",
    "    print(f\"no. relation size: {relations.shape}\")\n",
    "    print(f\"edge_index size: {data.edge_index.size()}\")\n",
    "    print(f\"min: {np.min(relations)}\")\n",
    "    print(f\"min: {np.max(relations)}\")\n",
    "    return dataset, le_entity, le_entity2, le_relation, le_relation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(sub, obj, rel):\n",
    "    sub = le_entity.inverse_transform(le_entity2.inverse_transform([sub]))\n",
    "    obj = le_entity.inverse_transform(le_entity2.inverse_transform([obj]))\n",
    "    rel = le_relation.inverse_transform(le_relation2.inverse_transform([rel]))\n",
    "    return sub[0], obj[0], rel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/m/07pd_j', '/m/02l7c8', '/film/film/genre')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_transform(data.edge_index[0][0], data.edge_index[1][0], data.edge_type[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict ={}\n",
    "for n1, n2, ys in zip(data.edge_index[0], data.edge_index[1], data.edge_type):\n",
    "    label_dict[int(n1), int(n2)] = int(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=True,\n",
    "                          cmap=\"Set2\")\n",
    "        nx.draw_networkx_edge_labels(h, pos=nx.spring_layout(h, seed=42), edge_labels=label_dict, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fafb7c00f946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\torch_geometric\\utils\\convert.py\u001b[0m in \u001b[0;36mto_networkx\u001b[1;34m(data, node_attrs, edge_attrs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\torch_geometric\\utils\\convert.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "G = to_networkx(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-72554d500022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(dataset.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you should probably not visualize huge node networks\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "if len(unique_entities) > 200:\n",
    "    print(\"you should probably not visualize huge node networks\")\n",
    "else:\n",
    "    visualize(G) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def freebase_parser(freebase_id):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    query = \\\n",
    "    '''SELECT ?sLabel WHERE { \n",
    "        ?s wdt:P646 \"''' + freebase_id + '''\".\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    LIMIT 1'''\n",
    "    res = get_results(endpoint_url, query)\n",
    "    if len(res['results']['bindings']) == 0:\n",
    "        return \"No result\"\n",
    "    else:\n",
    "        return get_results(endpoint_url, query)['results']['bindings'][0]['sLabel']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdf2txt(sub, obj, rel):\n",
    "    sub, obj, rel = inverse_transform(sub,obj,rel)\n",
    "    sub = freebase_parser(sub)\n",
    "    obj = freebase_parser(obj)\n",
    "    return sub, obj, str(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Racine',\n",
       " 'Lubbock',\n",
       " '/award/award_nominated_work/award_nominations./award/award_nomination/award')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf2txt(23, 2, 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c03de61a4e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnode2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use temp_folder for big graphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "# Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "node2vec = Node2Vec(G, dimensions=16, walk_length=15, num_walks=20, workers=1)  # Use temp_folder for big graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed nodes\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  \n",
    "# Any keywords acceptable by gensim.Word2Vec can be passed, \n",
    "# `dimensions` and `workers` are automatically passed\n",
    "# (from the Node2Vec constructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2Text(node_id):\n",
    "    freebase_id = le_entity.inverse_transform(le_entity2.inverse_transform([node_id]))\n",
    "    return freebase_parser(freebase_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for most similar nodes\n",
    "NODE_ID = '2'\n",
    "print(f\"Most similar Nodes to {node2Text(int(NODE_ID))}\")\n",
    "for node in model.wv.most_similar(NODE_ID):\n",
    "    sim_node_id, percentage = node\n",
    "    print(node2Text(int(sim_node_id)), percentage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for later use\n",
    "import os\n",
    "PATH = 'embeddings'\n",
    "NODE_EMBEDDING_NAME = \"node_embedding\"\n",
    "EMBEDDING_MODEL_NAME = \"node_embedding_model\"\n",
    "if not os.path.exists(PATH):\n",
    "    os.mkdir(PATH)\n",
    "model.wv.save_word2vec_format(os.path.join(PATH, NODE_EMBEDDING_NAME + \".kv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later use\n",
    "model.save(os.path.join(PATH, EMBEDDING_MODEL_NAME + \".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings \n",
    "embeddings = model.wv.load_word2vec_format(os.path.join(PATH, NODE_EMBEDDING_NAME + \".kv\"))\n",
    "embedded_model = model.wv.load(os.path.join(PATH, EMBEDDING_MODEL_NAME + \".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for most similar nodes\n",
    "NODE_ID = '2'\n",
    "print(f\"Most similar Nodes to {node2Text(int(NODE_ID))}\")\n",
    "for node in embedded_model.wv.most_similar(NODE_ID):\n",
    "    sim_node_id, percentage = node\n",
    "    print(node2Text(int(sim_node_id)), percentage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-c9768d42cce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedded_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embedded_x = torch.tensor(embeddings.vectors, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-f36b2849d581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.edge_index = torch.stack([row, col], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-3691167ed672>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGAE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\torch_geometric\\nn\\models\\autoencoder.py\u001b[0m in \u001b[0;36msplit_edges\u001b[1;34m(self, data, val_ratio, test_ratio)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;34m'batch'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m  \u001b[1;31m# No batch-mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = GAE.split_edges(GAE, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'test_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-a3d66273045f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_pos_edge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_pos_edge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Data' object has no attribute 'test_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "l1 = data.test_pos_edge_index[0][0]\n",
    "l2 = data.test_pos_edge_index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e53f0b27b37b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l1' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataset.edge_index[0].tolist().index(l1))\n",
    "print(dataset.edge_index[1].tolist().index(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(batch=[13292], edge_attr=[50000], edge_index=[2, 50000], edge_type=[50000], num_graphs=[1], x=[13292, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-1b862e394b4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5217\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10644\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y[np.where(dataset.edge_index.T == torch.tensor([5217, 10644]))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading FB15k training (valid file) data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_encode_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-42609ee015ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\tLoading FB15k training (valid file) data...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encode_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# create node embeddings if none exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_encode_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "\n",
    "entity_id = pd.read_csv('./data/FB15k/entities.txt', sep='\\t', header=None, names=['entity', 'id'], engine='python')\n",
    "entity = entity_id['entity'].values\n",
    "\n",
    "relation_id = pd.read_csv('./data/FB15k/relations.txt', sep='\\t', header=None, names=['relation', 'id'], engine='python')\n",
    "relation = relation_id['relation'].values\n",
    "\n",
    "data = pd.read_csv('./data/FB15k/valid.txt', sep='\\t', header=None, names=['subject', 'object', 'relation'], engine='python')\n",
    "print('\\tLoading FB15k training (valid file) data...')\n",
    "\n",
    "dataset = label_encode_dataset(entity, relation, data)\n",
    "\n",
    "# create node embeddings if none exists\n",
    "if not os.path.exists(\"embeddings\"):\n",
    "    create_node_embedding(dataset)\n",
    "embedded_nodes =  KeyedVectors.load_word2vec_format('embeddings/node_embedding.kv')\n",
    "\n",
    "dataset.x = torch.tensor(embedded_nodes.vectors, dtype=torch.float)\n",
    "data = GAE.split_edges(GAE, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "full_length = dataset.edge_index.shape[-1]\n",
    "train_index = torch.tensor(dataset.edge_index[:, 0:m.floor(full_length*0.7)], dtype=torch.long)\n",
    "train_attr_index = torch.tensor(dataset.edge_attr[0:m.floor(full_length*0.7)], dtype=torch.long)\n",
    "\n",
    "val_index = torch.tensor(dataset.edge_index[:, m.floor(full_length*0.7):m.floor(full_length*0.9)], dtype=torch.long)\n",
    "val_attr_index = torch.tensor(dataset.edge_attr[m.floor(full_length*0.7):m.floor(full_length*0.9)], dtype=torch.long)\n",
    "\n",
    "test_index = torch.tensor(dataset.edge_index[:, m.floor(full_length*0.9):], dtype=torch.long)\n",
    "test_attr_index = torch.tensor(dataset.edge_attr[m.floor(full_length*0.9):], dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "dataset.edge_index = torch.cat([train_index, val_index, test_index], dim=1)\n",
    "dataset.edge_attr = torch.cat([train_attr_index, val_attr_index, test_attr_index])\n",
    "\n",
    "dataset.edge_train_mask = torch.cat([torch.ones((train_index.size(-1))),\n",
    "                                  torch.zeros((val_index.size(-1))),\n",
    "                                  torch.zeros((test_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_val_mask = torch.cat([torch.zeros((train_index.size(-1))),\n",
    "                                torch.ones((val_index.size(-1))),\n",
    "                                torch.zeros((test_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_test_mask = torch.cat([torch.zeros((train_index.size(-1))),\n",
    "                                 torch.zeros((val_index.size(-1))),\n",
    "                                 torch.ones((test_index.size(-1)))], dim=0).byte()\n",
    "\n",
    "dataset.edge_train_attr_mask = torch.cat([torch.ones((train_attr_index.size(-1))),\n",
    "                                  torch.zeros((val_attr_index.size(-1))),\n",
    "                                  torch.zeros((test_attr_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_val_attr_mask = torch.cat([torch.zeros((train_attr_index.size(-1))),\n",
    "                                torch.ones((val_attr_index.size(-1))),\n",
    "                                torch.zeros((test_attr_index.size(-1)))], dim=0).byte()\n",
    "dataset.edge_test_attr_mask = torch.cat([torch.zeros((train_attr_index.size(-1))),\n",
    "                                 torch.zeros((val_attr_index.size(-1))),\n",
    "                                 torch.ones((test_attr_index.size(-1)))], dim=0).byte()\n",
    "\n",
    "dataset.edge_type = torch.zeros(((dataset.edge_index.size(-1)),)).long()\n",
    "\n",
    "dataset.batch = torch.zeros((1, dataset.num_nodes), dtype=torch.int64).view(-1)\n",
    "dataset.num_graphs = 1\n",
    "num_features = dataset.x.shape[-1] \n",
    "num_relations = max(np.unique(dataset.edge_attr)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'val_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-acd23f9ef62f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_pos_edge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\oddgu\\programmering\\multirelational-graphstar\\env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'val_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "np.where(dataset.edge_index.T == data.val_pos_edge_index.T[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'val_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-96523fa0b15f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_pos_edge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_pos_edge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Data' object has no attribute 'val_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "pair = dataset.val_pos_edge_index.T[0]\n",
    "obj = pair[1]\n",
    "sub = pair[0]\n",
    "rel = np.where(dataset.edge_index.T == data.val_pos_edge_index.T[0])[0]\n",
    "print(obj, sub, rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-5a208a4fb0ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdf2txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "rdf2txt(int(sub), int(obj), int(y[rel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(898.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_attr[4877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('American Pie', 'romance film', '/film/film/genre')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf2txt(8937, 4141, 341)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec\n",
    "import os\n",
    "\n",
    "def make_node_embeddings(dataset, path=\"embeddings\", node_embedding_name=\"node_embeddings\", embedding_model_name=\"node_embedding_model\", dimensions=16, walk_length=15, num_walks=20, workers=1, window=10, min_count=1, batch_words=4):\n",
    "    G = to_networkx(dataset)\n",
    "   \n",
    "    # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)  # Use temp_folder for big graphs\n",
    "    \n",
    "    # Embed nodes\n",
    "    model = node2vec.fit(window=window, min_count=min_count, batch_words=batch_words)  # Any keywords acceptable by gensim.Word2Vec can be passed, \n",
    "                                                                 # `dimensions` and `workers` are automatically passed\n",
    "                                                                 # (from the Node2Vec constructor)\n",
    "    \n",
    "    # Save embeddings for later use\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    model.wv.save_word2vec_format(os.path.join(path, node_embedding_name + \".kv\"))\n",
    "    model.save(os.path.join(path, embedding_model_name + \".pkl\"))\n",
    "    print(f\"Saved embedding and model in the {path} folder\")\n",
    "    return model.vw.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Graph Star Multi Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"output/FB15K_1024_Hid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphStar(\n",
       "  (fl): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (star_init): StarAttn(\n",
       "    (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv_list): ModuleList(\n",
       "    (0): GraphStarConv(256, 256, heads=4)\n",
       "    (1): GraphStarConv(256, 256, heads=4)\n",
       "    (2): GraphStarConv(256, 256, heads=4)\n",
       "  )\n",
       "  (star_attn_list): ModuleList(\n",
       "    (0): StarAttn(\n",
       "      (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): StarAttn(\n",
       "      (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): StarAttn(\n",
       "      (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (sLayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (gcl1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (gcl2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (gcl3): Linear(in_features=128, out_features=0, bias=True)\n",
       "  (rl): Linear(in_features=918, out_features=256, bias=True)\n",
       "  (LP_loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[50000], edge_index=[2, 50000], x=[13292, 16])\n",
      "no. unique relations: 916\n",
      "no. edge_type size: torch.Size([50000])\n",
      "no. relation size: (50000,)\n",
      "edge_index size: torch.Size([2, 50000])\n",
      "min: 0\n",
      "min: 915\n"
     ]
    }
   ],
   "source": [
    "dataset, le_entity, le_entity2, le_relation, le_relation2 = load()\n",
    "\n",
    "def inverse_transform(sub, obj, rel):\n",
    "    sub = le_entity.inverse_transform(le_entity2.inverse_transform([sub]))\n",
    "    obj = le_entity.inverse_transform(le_entity2.inverse_transform([obj]))\n",
    "    rel = le_relation.inverse_transform(le_relation2.inverse_transform([rel]))\n",
    "    return sub[0], obj[0], rel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits embedding [13292, 256] -> [unique nodes x hidden layer]\n",
    "z = model.z\n",
    "# Edge index between 2 nodes\n",
    "edge_index = dataset.edge_index.T[100].T\n",
    "# relation type\n",
    "edge_type = dataset.edge_type[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=918, out_features=256, bias=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Syria', 'Damascus', '/location/country/administrative_divisions')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf2txt(edge_index[0], edge_index[1], edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0461, -0.0483, -0.0591,  ..., -0.0494, -0.0815,  0.0566],\n",
       "        [-0.0573, -0.0042,  0.0070,  ..., -0.0586, -0.0540, -0.0835],\n",
       "        [-0.0400, -0.0135, -0.0529,  ..., -0.0017, -0.0051,  0.0231],\n",
       "        ...,\n",
       "        [ 0.0662,  0.0853,  0.0700,  ...,  0.0435, -0.0358, -0.0923],\n",
       "        [-0.0007, -0.0064,  0.0640,  ..., -0.0492, -0.0112,  0.0284],\n",
       "        [-0.0126,  0.0563, -0.0568,  ..., -0.0611, -0.0003, -0.0608]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updated in training (not after)\n",
    "model.RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = z[edge_index[0]]\n",
    "relation =  model.RW\n",
    "tail = z[edge_index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([918, 256])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.RW.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([918, 256])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING FUNCTION\n",
    "p = head*relation*tail\n",
    "score = torch.sigmoid(p.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([918, 256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(head.size())\n",
    "print(relation.size())\n",
    "print(tail.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = score.detach().numpy().argsort()[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([521, 719,  84, 646, 826, 618, 306, 285, 167, 169], dtype=int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Syria', 'Damascus', '/location/location/partially_containedby')\n",
      "('Syria', 'Damascus', '/people/person/spouse_s./people/marriage/type_of_union')\n",
      "('Syria', 'Damascus', '/base/biblioness/bibs_location/country')\n",
      "('Syria', 'Damascus', '/olympics/olympic_sport/athletes./olympics/olympic_athlete_affiliation/athlete')\n",
      "('Syria', 'Damascus', '/time/event/locations')\n",
      "('Syria', 'Damascus', '/music/performance_role/track_performances./music/track_contribution/role')\n",
      "('Syria', 'Damascus', '/fictional_universe/fictional_character/occupation')\n",
      "('Syria', 'Damascus', '/education/field_of_study/students_majoring./education/education/minor')\n",
      "('Syria', 'Damascus', '/baseball/baseball_player/former_teams./baseball/baseball_historical_roster_position/team')\n",
      "('Syria', 'Damascus', '/baseball/baseball_player/position_s')\n"
     ]
    }
   ],
   "source": [
    "for l in pred:\n",
    "    print(rdf2txt(edge_index[0], edge_index[1], l))\n",
    "# Recall precision on top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits embedding [13292, 256] -> [unique nodes x hidden layer]\n",
    "z = model.z\n",
    "\n",
    "def experiment(index=0):\n",
    "    \n",
    "    # Edge index between 2 nodes\n",
    "    edge_index = dataset.edge_index.T[index].T\n",
    "    # relation type\n",
    "    edge_type = dataset.edge_type[index]\n",
    "    \n",
    "    h, t, r = rdf2txt(edge_index[0], edge_index[1], edge_type)\n",
    "    print(f\" \\\n",
    "        Original data: \\n \\\n",
    "        Head: {h} \\n \\\n",
    "        Relation: {r} \\n \\\n",
    "        Tail: {t} \\n\")\n",
    "    \n",
    "    head = z[edge_index[0]]\n",
    "    relation =  model.RW\n",
    "    tail = z[edge_index[1]]\n",
    "    p = head * relation * tail\n",
    "    pred = int(round(np.argmax(p.detach().numpy())/256))\n",
    "    \n",
    "    h, t, r = rdf2txt(edge_index[0], edge_index[1], pred)\n",
    "    print(f\" \\\n",
    "        Predicted data: \\n \\\n",
    "        Head: {h} \\n \\\n",
    "        Relation: {r} \\n \\\n",
    "        Tail: {t} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Original data: \n",
      "         Head: As Good as It Gets \n",
      "         Relation: /film/film/other_crew./film/film_crew_gig/film_crew_role \n",
      "         Tail: make-up artist \n",
      "\n",
      "         Predicted data: \n",
      "         Head: As Good as It Gets \n",
      "         Relation: /cvg/cvg_genre/games \n",
      "         Tail: make-up artist \n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment(1690)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = z[edge_index[0]]\n",
    "relation =  model.RW[edge_type]\n",
    "tail = z[edge_index[1]]\n",
    "p = head * relation * tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5769, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate\n",
    "All should use top 10 / 5 / 1\n",
    "*  Zero - One f0-score (does it predict correctly or not)\n",
    "*  Hierachical correctness (How far up the relationtree ? Does it get film/film or film/film/other_crew)\n",
    "*  Top-K hits (Is the correct in label in top-K in predictions) (precision, recall, f0)\n",
    "*  Split relationship and regex in pred relations (finding related relations)\n",
    "\n",
    "Need to explain hierachical layout of freebase!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
